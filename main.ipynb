{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💬 ChipChat - 데이터시트 챗봇\n",
    "\n",
    "**전처리된 데이터시트 JSON 파일을 기반으로 질의응답을 수행하는 챗봇입니다.**\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 시작하기 전에\n",
    "\n",
    "이 노트북은 ChipChat 앱을 실행하는 방법을 제공합니다.\n",
    "로컬 환경과 Google Colab 환경 모두에서 사용할 수 있습니다.\n",
    "\n",
    "## ✨ 주요 기능들\n",
    "- 🤖 **AI Agent 시스템**: LangGraph 기반 스마트 에이전트\n",
    "- 🔧 **3가지 Tool 자동 선택**: chipDB 검색, 벡터스토어 검색, PDF 처리\n",
    "- 📊 **ChipDB.csv 연동**: 부품 사양 요약 데이터베이스 활용\n",
    "- 🎯 **다중 LLM 지원**: OpenAI와 Claude 모델 선택 가능\n",
    "- 📄 **실시간 PDF 업로드**: 새 데이터시트 자동 처리 및 통합\n",
    "- 🔍 **고급 필터링**: 부품번호, 제조사, 카테고리별 검색\n",
    "- 🛠️ **프롬프트 커스터마이징**: 시스템 프롬프트 자유 수정\n",
    "- 🏷️ **메타데이터 추적**: 소스 정보 표시\n",
    "\n",
    "## 📋 사용 방법\n",
    "아래 셀들을 **순서대로 실행**만 하면 됩니다. (각 셀의 ▶️ 버튼을 클릭)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ✅ 1단계: 환경 설정 { display-mode: \"form\" }\n",
    "#@markdown 로컬 또는 Google Drive 환경을 선택하고 설정을 관리합니다.\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 환경 선택 토글\n",
    "use_google_drive = False #@param {type:\"boolean\"}\n",
    "#@markdown ✅ 체크: Google Drive 사용 (Colab 환경) | ❌ 체크해제: 로컬 환경 사용\n",
    "\n",
    "# config.json 파일 경로\n",
    "config_file = Path('prep/config.json')\n",
    "\n",
    "if config_file.exists():\n",
    "    # 기존 config.json 로드\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # 환경 설정 업데이트\n",
    "    config['environment']['use_google_drive'] = use_google_drive\n",
    "    \n",
    "    # 업데이트된 설정 저장\n",
    "    with open(config_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(\"✅ config.json 업데이트 완료!\")\n",
    "else:\n",
    "    print(\"❌ config.json 파일이 없습니다. 기본 설정을 생성합니다.\")\n",
    "    \n",
    "    # 기본 config.json 생성\n",
    "    default_config = {\n",
    "        \"environment\": {\n",
    "            \"use_google_drive\": use_google_drive,\n",
    "            \"description\": \"Environment selection: true for Google Drive (Colab), false for local\"\n",
    "        },\n",
    "        \"paths\": {\n",
    "            \"google_drive\": {\n",
    "                \"base_path\": \"/content/drive/MyDrive\",\n",
    "                \"prep_json_folder\": \"/content/drive/MyDrive/prep_json\",\n",
    "                \"vectorstore_folder\": \"/content/drive/MyDrive/vectorstore\",\n",
    "                \"prompt_templates_folder\": \"/content/drive/MyDrive/prompts\",\n",
    "                \"model_cache_folder\": \"/content/drive/MyDrive/hf_model_cache\",\n",
    "                \"logs_folder\": \"/content/drive/MyDrive/chipchat_logs\",\n",
    "                \"prep_datasheets_folder\": \"/content/drive/MyDrive/datasheets\",\n",
    "                \"prep_prep_json_folder\": \"/content/drive/MyDrive/prep_json\",\n",
    "                \"prep_vectorstore_folder\": \"/content/drive/MyDrive/vectorstore\"\n",
    "            },\n",
    "                    \"local\": {\n",
    "            \"base_path\": \".\",\n",
    "            \"prep_json_folder\": \"./prep/prep_json\",\n",
    "            \"vectorstore_folder\": \"./prep/vectorstore\",\n",
    "            \"prompt_templates_folder\": \"./prompts\",\n",
    "            \"model_cache_folder\": \"./hf_model_cache\",\n",
    "            \"logs_folder\": \"./logs\",\n",
    "            \"prep_datasheets_folder\": \"./prep/datasheets\",\n",
    "            \"prep_prep_json_folder\": \"./prep/prep_json\",\n",
    "            \"prep_vectorstore_folder\": \"./prep/vectorstore\"\n",
    "        }\n",
    "        },\n",
    "        \"vectorstore\": {\n",
    "            \"default_name\": \"datasheet_vectors_final\"\n",
    "        },\n",
    "        \"models\": {\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"supported_llm\": {\n",
    "                \"openai\": [\n",
    "                    \"gpt-4o-mini\",\n",
    "                    \"gpt-4o\",\n",
    "                    \"gpt-3.5-turbo\"\n",
    "                ],\n",
    "                \"claude\": [\n",
    "                    \"claude-3-sonnet-20240229\",\n",
    "                    \"claude-3-haiku-20240307\",\n",
    "                    \"claude-3-opus-20240229\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"pdf_processing\": {\n",
    "            \"pages_per_chunk\": 3,\n",
    "            \"chunk_overlap\": 1,\n",
    "            \"output_formats\": {\n",
    "                \"save_filtered_pdf\": True,\n",
    "                \"save_summary_only\": False,\n",
    "                \"save_combined\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(config_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(default_config, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    config = default_config\n",
    "    print(\"✅ 기본 config.json 생성 완료!\")\n",
    "\n",
    "# 현재 환경 설정 표시\n",
    "env_type = \"Google Drive\" if use_google_drive else \"로컬\"\n",
    "current_paths = config['paths']['google_drive' if use_google_drive else 'local']\n",
    "\n",
    "print(f\"\\n🌐 {env_type} 환경으로 설정되었습니다!\")\n",
    "print(f\"📋 설정된 경로들:\")\n",
    "for key, path in current_paths.items():\n",
    "    if key != 'base_path':\n",
    "        print(f\"  • {key.replace('_', ' ').title()}: {path}\")\n",
    "\n",
    "# Google Drive 마운트 (필요한 경우)\n",
    "if use_google_drive:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"\\n🔄 Google Drive 마운트 중...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"✅ Google Drive 연동 완료!\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Google Colab 환경이 아닙니다. 환경 설정을 로컬로 변경합니다.\")\n",
    "        config['environment']['use_google_drive'] = False\n",
    "        with open(config_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "        current_paths = config['paths']['local']\n",
    "        print(\"💻 로컬 환경으로 전환되었습니다.\")\n",
    "\n",
    "# 필요한 디렉토리 생성\n",
    "print(\"\\n📁 필요한 디렉토리 생성 중...\")\n",
    "for key, path_str in current_paths.items():\n",
    "    if key != 'base_path':\n",
    "        path = Path(path_str)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  ✅ {path}\")\n",
    "\n",
    "print(\"\\n🎉 환경 설정이 완료되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 📥 2단계: 필요한 라이브러리 설치 { display-mode: \"form\" }\n",
    "#@markdown 필요한 라이브러리들을 설치합니다.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# config.json에서 환경 설정 읽기\n",
    "config_file = Path('prep/config.json')\n",
    "use_google_drive = False\n",
    "\n",
    "if config_file.exists():\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    use_google_drive = config.get('environment', {}).get('use_google_drive', False)\n",
    "\n",
    "print(\"📥 필요한 라이브러리 설치 중...\")\n",
    "\n",
    "# 환경별 설치 과정\n",
    "if use_google_drive:\n",
    "    print(\"🌐 Google Colab 환경 설정...\")\n",
    "    \n",
    "    # GitHub 저장소 클론 (Colab 환경)\n",
    "    try:\n",
    "        if not Path('src').exists():  # src 폴더가 없으면 클론 필요\n",
    "            print(\"🔄 GitHub 저장소 클론 중...\")\n",
    "            subprocess.run(['git', 'clone', 'https://github.com/doyoung42/chipchat_demo.git', 'temp_clone'], check=True)\n",
    "            \n",
    "            # 필요한 파일들 복사\n",
    "            import shutil\n",
    "            if Path('temp_clone/src').exists():\n",
    "                shutil.copytree('temp_clone/src', 'src', dirs_exist_ok=True)\n",
    "            if Path('temp_clone/requirements.txt').exists():\n",
    "                shutil.copy2('temp_clone/requirements.txt', 'requirements.txt')\n",
    "            \n",
    "            # 임시 폴더 삭제\n",
    "            shutil.rmtree('temp_clone')\n",
    "            print(\"✅ GitHub 저장소 클론 및 파일 복사 완료\")\n",
    "        else:\n",
    "            print(\"✅ 소스 코드 이미 존재\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GitHub 클론 실패: {str(e)}\")\n",
    "    \n",
    "    # Colab 전용 패키지 설치\n",
    "    try:\n",
    "        subprocess.run(['pip', 'install', 'pyngrok==7.0.1', '-q'], check=True)\n",
    "        print(\"✅ pyngrok (외부 접속용) 설치 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ pyngrok 설치 실패: {str(e)}\")\n",
    "\n",
    "else:\n",
    "    print(\"💻 로컬 환경 설정...\")\n",
    "    \n",
    "    # 로컬 환경에서는 이미 git clone되어 있다고 가정\n",
    "    if Path('src').exists():\n",
    "        print(\"✅ 소스 코드 확인 완료 (로컬 저장소)\")\n",
    "    else:\n",
    "        print(\"❌ src 폴더를 찾을 수 없습니다.\")\n",
    "        print(\"💡 GitHub에서 저장소를 clone했는지 확인해주세요.\")\n",
    "        print(\"   git clone https://github.com/doyoung42/chipchat_demo.git\")\n",
    "\n",
    "# requirements.txt 설치\n",
    "if use_google_drive:\n",
    "    if Path('requirements.txt').exists():\n",
    "        try:\n",
    "            subprocess.run(['pip', 'install', '-r', 'requirements.txt', '-q'], check=True)\n",
    "            print(\"✅ Requirements 설치 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Requirements 설치 실패: {str(e)}\")\n",
    "    else:\n",
    "        print(\"⚠️ requirements.txt 파일을 찾을 수 없습니다.\")\n",
    "        print(\"💡 기본 라이브러리들만 사용합니다.\")\n",
    "\n",
    "# Python path 설정\n",
    "import sys\n",
    "current_dir = Path(os.getcwd())\n",
    "sys.path.append(str(current_dir))\n",
    "sys.path.append(str(current_dir / 'src'))\n",
    "\n",
    "# 새로운 디렉토리 구조 추가\n",
    "src_dir = current_dir / 'src'\n",
    "if src_dir.exists():\n",
    "    sys.path.append(str(src_dir))\n",
    "    for subdir in ['config', 'models', 'utils', 'app']:\n",
    "        subdir_path = src_dir / subdir\n",
    "        if subdir_path.exists():\n",
    "            sys.path.append(str(subdir_path))\n",
    "\n",
    "print(\"\\n✅ 라이브러리 설치 및 경로 설정 완료!\")\n",
    "print(\"\\n📋 지원되는 LLM 모델:\")\n",
    "print(\"🔸 OpenAI: gpt-4o-mini, gpt-4o, gpt-3.5-turbo\")\n",
    "print(\"🔸 Claude: claude-3-sonnet, claude-3-haiku, claude-3-opus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 🚀 2-1단계: 로깅 시스템 초기화 및 모델 사전 다운로드 { display-mode: \"form\" }\n",
    "#@markdown HuggingFace 모델을 캐싱하여 빠른 로딩을 지원합니다.\n",
    "\n",
    "import time\n",
    "from src.utils.logger import get_logger\n",
    "from src.utils.model_cache import get_model_cache\n",
    "\n",
    "# 로깅 시스템 초기화\n",
    "print(\"📊 로깅 시스템 초기화 중...\")\n",
    "logger = get_logger()\n",
    "logger.log_system_info()\n",
    "logger.info(\"Main notebook 실행 시작\")\n",
    "\n",
    "# 모델 캐시 시스템 초기화\n",
    "print(\"\\n🗄️ 모델 캐시 시스템 초기화 중...\")\n",
    "model_cache = get_model_cache()\n",
    "cache_info = model_cache.get_cache_info()\n",
    "\n",
    "print(f\"📁 캐시 디렉토리: {cache_info['cache_dir']}\")\n",
    "print(f\"📦 캐시된 모델 수: {cache_info['model_count']}\")\n",
    "print(f\"💾 전체 캐시 크기: {cache_info['total_size_mb']:.2f} MB\")\n",
    "\n",
    "if cache_info['models']:\n",
    "    print(\"\\n✅ 캐시된 모델:\")\n",
    "    for model in cache_info['models']:\n",
    "        print(f\"  - {model}\")\n",
    "\n",
    "# 모델 사전 다운로드 옵션\n",
    "download_model = True #@param {type:\"boolean\"}\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" #@param {type:\"string\"}\n",
    "\n",
    "if download_model:\n",
    "    print(f\"\\n🔄 모델 다운로드 또는 캐시 확인: {model_name}\")\n",
    "    \n",
    "    @logger.measure_time(\"모델 준비\")\n",
    "    def prepare_model():\n",
    "        # 캐시 확인\n",
    "        if model_cache.is_model_cached(model_name):\n",
    "            print(\"✅ 모델이 이미 캐시되어 있습니다.\")\n",
    "            # 캐시에서 로드\n",
    "            if model_cache.load_model_from_cache(model_name):\n",
    "                print(\"✅ 캐시에서 모델 로드 완료!\")\n",
    "                return True\n",
    "        \n",
    "        # 캐시에 없으면 다운로드\n",
    "        print(\"📥 모델을 다운로드합니다...\")\n",
    "        try:\n",
    "            from langchain_huggingface import HuggingFaceEmbeddings\n",
    "            \n",
    "            # 모델 다운로드 (초기화를 통해)\n",
    "            start_time = time.time()\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={'device': 'cpu'}\n",
    "            )\n",
    "            download_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"✅ 모델 다운로드 완료! (소요 시간: {download_time:.2f}초)\")\n",
    "            \n",
    "            # 캐시에 저장\n",
    "            print(\"💾 모델을 캐시에 저장 중...\")\n",
    "            if model_cache.save_model_to_cache(model_name):\n",
    "                pass  # save_model_to_cache에서 이미 저장 위치 정보를 출력함\n",
    "            else:\n",
    "                print(\"⚠️ 모델 캐시 저장 실패 (다음 실행 시 다시 다운로드됩니다)\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"모델 준비 실패: {e}\")\n",
    "            print(f\"❌ 모델 준비 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # 모델 준비 실행\n",
    "    success = prepare_model()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 모델 준비 완료! 이제 Streamlit 앱이 더 빠르게 로드됩니다.\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ 모델 준비에 실패했습니다. 하지만 계속 진행할 수 있습니다.\")\n",
    "\n",
    "# 성능 요약 표시\n",
    "print(\"\\n📊 현재까지의 성능 요약:\")\n",
    "perf_summary = logger.get_performance_summary()\n",
    "for op, stats in perf_summary.items():\n",
    "    print(f\"\\n🔹 {op}:\")\n",
    "    print(f\"   - 평균 시간: {stats['avg_time']:.2f}초\")\n",
    "    print(f\"   - 성공: {stats['success_count']}회, 실패: {stats['fail_count']}회\")\n",
    "\n",
    "print(\"\\n✅ 모든 준비가 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 🔑 3단계: API 키 설정 { display-mode: \"form\" }\n",
    "#@markdown AI 서비스의 API 키를 입력하여 챗봇을 설정합니다.\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "openai_api_key = \"\" #@param {type:\"string\"}\n",
    "claude_api_key = \"\" #@param {type:\"string\"}\n",
    "hf_token = \"\" #@param {type:\"string\"}\n",
    "\n",
    "# API 키 유효성 검사\n",
    "if not openai_api_key and not claude_api_key and not hf_token:\n",
    "    print(\"⚠️ API 키가 입력되지 않았습니다.\")\n",
    "    print(\"💡 최소 하나 이상의 API 키를 입력해주세요.\")\n",
    "    print(\"• OpenAI API 키: https://platform.openai.com/api-keys\")\n",
    "    print(\"• Claude API 키: https://console.anthropic.com/keys\")\n",
    "    print(\"• HuggingFace 토큰: https://huggingface.co/settings/tokens\")\n",
    "else:\n",
    "    # 환경 변수 설정\n",
    "    if openai_api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "        print(\"✅ OpenAI API 키 설정 완료!\")\n",
    "        \n",
    "    if claude_api_key:\n",
    "        os.environ[\"ANTHROPIC_API_KEY\"] = claude_api_key\n",
    "        print(\"✅ Claude API 키 설정 완료!\")\n",
    "    \n",
    "    if hf_token:\n",
    "        os.environ[\"HF_TOKEN\"] = hf_token\n",
    "        print(\"✅ HuggingFace 토큰 설정 완료!\")\n",
    "    \n",
    "    # Streamlit 시크릿 파일 생성\n",
    "    secrets_dir = Path(\".streamlit\")\n",
    "    secrets_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    secrets = {}\n",
    "    if openai_api_key:\n",
    "        secrets[\"openai_api_key\"] = openai_api_key\n",
    "    if claude_api_key:\n",
    "        secrets[\"anthropic_api_key\"] = claude_api_key\n",
    "    if hf_token:\n",
    "        secrets[\"hf_token\"] = hf_token\n",
    "    \n",
    "    with open(secrets_dir / \"secrets.toml\", \"w\") as f:\n",
    "        for key, value in secrets.items():\n",
    "            f.write(f'{key} = \"{value}\"\\n')\n",
    "    \n",
    "    # 토큰 저장\n",
    "    try:\n",
    "        from src.config.token_manager import TokenManager\n",
    "        token_manager = TokenManager()\n",
    "        \n",
    "        if openai_api_key:\n",
    "            token_manager.set_token('openai', openai_api_key)\n",
    "        \n",
    "        if claude_api_key:\n",
    "            token_manager.set_token('anthropic', claude_api_key)\n",
    "        \n",
    "        if hf_token:\n",
    "            token_manager.set_token('huggingface', hf_token)\n",
    "            \n",
    "        print(\"✅ 토큰 관리자에 API 키 저장 완료!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 토큰 저장 중 오류가 발생했습니다: {str(e)}\")\n",
    "    \n",
    "    # 프롬프트 시스템 초기화 (이제 PromptManager가 자동으로 처리)\n",
    "    print(\"✅ 프롬프트 시스템이 초기화되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 📊 4단계: Streamlit 서버 실행 (로컬 최적화) { display-mode: \"form\" }\n",
    "#@markdown 로컬 환경에 최적화된 간단한 Streamlit 실행\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 환경 확인\n",
    "config_file = Path('prep/config.json')\n",
    "is_colab_env = False\n",
    "\n",
    "if config_file.exists():\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    is_colab_env = config.get('environment', {}).get('use_google_drive', False)\n",
    "\n",
    "# 앱 설정\n",
    "app_file = \"src/app/streamlit_app.py\"\n",
    "port = 8501\n",
    "\n",
    "print(f\"🚀 ChipChat {'(Google Colab)' if is_colab_env else '(로컬)'} 시작 중...\")\n",
    "print(f\"📁 파일: {app_file}\")\n",
    "print(f\"🌐 포트: {port}\")\n",
    "print(f\"✨ 주요 기능: AI 에이전트, 멀티턴 대화, LLM 모델 선택\")\n",
    "\n",
    "# 환경별 처리\n",
    "if not is_colab_env:\n",
    "    # 로컬 환경 (간소화)\n",
    "    print(\"\\n💻 로컬 환경에서 실행합니다.\")\n",
    "    \n",
    "    # 기존 프로세스 정리 (조용히)\n",
    "    try:\n",
    "        subprocess.run(['pkill', '-f', 'streamlit'], \n",
    "                      check=False, capture_output=True, text=True)\n",
    "        time.sleep(1)\n",
    "        print(\"✅ 기존 프로세스 정리 완료\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 간단한 Streamlit 명령어\n",
    "    cmd = [\n",
    "        \"streamlit\", \"run\", \n",
    "        app_file,\n",
    "        f\"--server.port={port}\", \n",
    "        \"--server.address=localhost\",\n",
    "        \"--server.headless=true\",\n",
    "        \"--browser.gatherUsageStats=false\",\n",
    "        \"--logger.level=warning\"  # 로그 최소화\n",
    "    ]\n",
    "    \n",
    "    print(\"🔧 Streamlit 시작 중...\")\n",
    "    \n",
    "    # 로컬 환경에서도 기본 경로 설정 (오류 방지용)\n",
    "    try:\n",
    "        from src.utils.config_manager import get_config_manager\n",
    "        config = get_config_manager()\n",
    "        vectorstore_folder = config.get_path('vectorstore_folder')\n",
    "        prep_json_folder = config.get_path('prep_json_folder')\n",
    "        prompt_templates_folder = config.get_path('prompt_templates_folder')\n",
    "    except:\n",
    "        vectorstore_folder = './vectorstore'\n",
    "        prep_json_folder = './prep_json'\n",
    "        prompt_templates_folder = './prompt_templates'\n",
    "\n",
    "    # 환경 변수 설정\n",
    "    os.environ['VECTORSTORE_PATH'] = str(vectorstore_folder)\n",
    "    os.environ['JSON_FOLDER_PATH'] = str(prep_json_folder)\n",
    "    os.environ['PROMPT_TEMPLATES_PATH'] = str(prompt_templates_folder)\n",
    "    \n",
    "    try:\n",
    "        # 백그라운드 실행 (로그 최소화)\n",
    "        process = subprocess.Popen(\n",
    "            cmd, \n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # 잠시 대기\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # 상태 확인\n",
    "        if process.poll() is None:  # 실행 중\n",
    "            print(\"✅ ChipChat이 성공적으로 시작되었습니다!\")\n",
    "            print(f\"\\n🔗 브라우저에서 다음 주소로 접속하세요:\")\n",
    "            print(f\"🌐 http://localhost:{port}\")\n",
    "            print(f\"\\n💡 사용 방법:\")\n",
    "            print(f\"  • 설정 페이지에서 LLM 모델을 선택하세요\")\n",
    "            print(f\"  • AI 에이전트가 자동으로 도구를 선택합니다\")\n",
    "            print(f\"  • 종료하려면 5단계를 실행하세요\")\n",
    "            \n",
    "            # 프로세스 저장 (5단계에서 사용)\n",
    "            globals()['streamlit_process'] = process\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ Streamlit 시작에 실패했습니다.\")\n",
    "            print(\"💡 다음을 확인해보세요:\")\n",
    "            print(\"  • requirements.txt가 설치되었는지\")\n",
    "            print(\"  • API 키가 설정되었는지\")\n",
    "            print(\"  • src 폴더가 존재하는지\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 실행 중 오류: {e}\")\n",
    "        print(\"💡 수동으로 실행해보세요:\")\n",
    "        print(f\"   streamlit run {app_file} --server.port={port}\")\n",
    "\n",
    "else:\n",
    "    # Google Colab 환경 (기존 복잡한 로직 실행)\n",
    "    print(\"🌐 Google Colab 환경에서 실행합니다.\")\n",
    "    print(\"⚠️ Google Colab 기능은 현재 버전에서 간소화되었습니다.\")\n",
    "    print(\"💡 수동으로 실행하거나 이전 버전을 사용해주세요.\")\n",
    "    \n",
    "    # 기본 변수 설정 (오류 방지용)\n",
    "    vectorstore_folder = './vectorstore'\n",
    "    prep_json_folder = './prep_json'\n",
    "    prompt_templates_folder = './prompt_templates'\n",
    "    \n",
    "    # 프로세스 저장 (5단계에서 사용)\n",
    "    globals()['streamlit_process'] = None\n",
    "    \n",
    "    print(\"💡 Google Colab에서 수동 실행하려면:\")\n",
    "    print(f\"   streamlit run {app_file} --server.port={port}\")\n",
    "\n",
    "# 4단계 완료 - 환경별 분기로 처리됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 🛑 5단계: 서버 중지 { display-mode: \"form\" }\n",
    "#@markdown 작업을 마치면 서버를 중지합니다.\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# 서버 중지 여부 확인\n",
    "stop_server = True  #@param {type:\"boolean\"}\n",
    "\n",
    "if stop_server:\n",
    "    print(\"🛑 서버 종료 프로세스를 시작합니다...\")\n",
    "    \n",
    "    # ngrok 터널 종료 (ngrok 사용한 경우에만)\n",
    "    try:\n",
    "        from pyngrok import ngrok\n",
    "        ngrok.kill()\n",
    "        print(\"✅ ngrok 터널이 종료되었습니다.\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"ℹ️ ngrok 종료: {str(e)}\")\n",
    "    \n",
    "    # 4단계에서 생성된 프로세스 먼저 종료\n",
    "    terminated_process = False\n",
    "    if 'streamlit_process' in globals():\n",
    "        try:\n",
    "            process = globals()['streamlit_process']\n",
    "            if process and process.poll() is None:\n",
    "                print(\"🔄 4단계에서 실행된 Streamlit 프로세스를 종료 중...\")\n",
    "                process.terminate()\n",
    "                time.sleep(3)\n",
    "                \n",
    "                if process.poll() is None:\n",
    "                    print(\"⚡ 강제 종료를 시도합니다...\")\n",
    "                    process.kill()\n",
    "                    time.sleep(2)\n",
    "                \n",
    "                print(\"✅ Streamlit 프로세스가 정상적으로 종료되었습니다.\")\n",
    "                terminated_process = True\n",
    "            else:\n",
    "                print(\"ℹ️ Streamlit 프로세스가 이미 종료되었습니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 프로세스 종료 중 오류: {e}\")\n",
    "    \n",
    "    # 추가적으로 모든 streamlit 프로세스 종료\n",
    "    try:\n",
    "        print(\"🔍 남은 Streamlit 프로세스를 검색 중...\")\n",
    "        result = subprocess.run(['pgrep', '-f', 'streamlit'], capture_output=True, text=True)\n",
    "        if result.stdout.strip():\n",
    "            pids = result.stdout.strip().split('\\n')\n",
    "            print(f\"📋 발견된 Streamlit 프로세스: {len(pids)}개\")\n",
    "            \n",
    "            subprocess.run(['pkill', '-f', 'streamlit'], check=False)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            result_after = subprocess.run(['pgrep', '-f', 'streamlit'], capture_output=True, text=True)\n",
    "            if not result_after.stdout.strip():\n",
    "                print(\"✅ 모든 Streamlit 프로세스가 정리되었습니다.\")\n",
    "            else:\n",
    "                print(\"⚠️ 일부 프로세스가 여전히 실행 중일 수 있습니다.\")\n",
    "        else:\n",
    "            print(\"✅ 실행 중인 Streamlit 프로세스가 없습니다.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 프로세스 정리 중 오류가 발생했습니다: {e}\")\n",
    "        print(\"💡 수동으로 런타임을 재시작하여 서버를 종료할 수 있습니다.\")\n",
    "    \n",
    "    # 포트 사용 상태 확인\n",
    "    try:\n",
    "        import socket\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        result = sock.connect_ex(('localhost', 8501))\n",
    "        sock.close()\n",
    "        \n",
    "        if result != 0:\n",
    "            print(\"✅ 포트 8501이 해제되었습니다.\")\n",
    "        else:\n",
    "            print(\"⚠️ 포트 8501이 아직 사용 중입니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ℹ️ 포트 상태 확인 실패: {e}\")\n",
    "    \n",
    "    print(\"\\n📋 서버 종료 완료!\")\n",
    "    print(\"💡 완전한 정리를 위해 런타임을 재시작하는 것을 권장합니다.\")\n",
    "    print(\"🔄 런타임 → 세션 재시작을 통해 모든 프로세스를 완전히 종료할 수 있습니다.\")\n",
    "else:\n",
    "    print(\"ℹ️ 서버 중지가 취소되었습니다.\")\n",
    "    print(\"💡 서버를 중지하려면 위의 체크박스를 선택하고 다시 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ 문제 해결\n",
    "\n",
    "**애플리케이션이 실행되지 않는 경우:**\n",
    "\n",
    "1. **API 키 확인**: OpenAI API 키가 올바르게 입력되었는지 확인\n",
    "2. **세션 재시작**: 런타임 → 세션 재시작 후 처음부터 다시 실행\n",
    "3. **JSON 파일 확인**: 전처리된 JSON 파일이 올바른 경로에 존재하는지 확인\n",
    "\n",
    "**사용 중 문제가 발생하는 경우:**\n",
    "- 브라우저를 새로고침하세요\n",
    "- 네트워크 연결 상태를 확인하세요\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
