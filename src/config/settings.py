"""
Configuration settings for the ChipChat application.
Supports both local and Google Colab environments.
"""
import os
import json
from pathlib import Path
from typing import Optional

def get_project_settings():
    """í”„ë¡œì íŠ¸ ì„¤ì •ì„ config.jsonì—ì„œ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        from ..utils.config_manager import get_config_manager
        config = get_config_manager()
        
        # í˜„ìž¬ í™˜ê²½ì˜ ê²½ë¡œë“¤ ê°€ì ¸ì˜¤ê¸°
        paths = config.get_paths()
        
        return {
            'BASE_DIR': Path(paths.get('base_path', '.')),
            'PREP_JSON_FOLDER': Path(paths.get('prep_json_folder', './prep_json')),
            'VECTORSTORE_FOLDER': Path(paths.get('vectorstore_folder', './vectorstore')), 
            'PROMPT_TEMPLATES_FOLDER': Path(paths.get('prompt_templates_folder', './prompt_templates')),
            'MODEL_CACHE_FOLDER': Path(paths.get('model_cache_folder', './hf_model_cache')),
            'LOGS_FOLDER': Path(paths.get('logs_folder', './logs')),
            'EMBEDDING_MODEL': config.get_embedding_model(),
            'SUPPORTED_MODELS': config.get_supported_models(),
            'USE_GOOGLE_DRIVE': config.get_environment() == 'google_drive'
        }
    except Exception as e:
        # config.json ì½ê¸° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        print(f"Warning: Failed to read config.json: {e}")
        return get_default_settings()

def get_default_settings():
    """ê¸°ë³¸ ì„¤ì •ê°’ ë°˜í™˜ (config.json ì½ê¸° ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)"""
    # í™˜ê²½ ê°ì§€
    try:
        from google.colab import drive
        # Google Colab í™˜ê²½
        base_dir = Path('/content/drive/MyDrive')
        use_google_drive = True
    except ImportError:
        # ë¡œì»¬ í™˜ê²½
        base_dir = Path('.')
        use_google_drive = False
    
    return {
        'BASE_DIR': base_dir,
        'PREP_JSON_FOLDER': base_dir / 'prep_json',
        'VECTORSTORE_FOLDER': base_dir / 'vectorstore',
        'PROMPT_TEMPLATES_FOLDER': base_dir / 'prompt_templates',
        'MODEL_CACHE_FOLDER': base_dir / 'hf_model_cache',
        'LOGS_FOLDER': base_dir / ('chipchat_logs' if use_google_drive else 'logs'),
        'EMBEDDING_MODEL': 'sentence-transformers/all-MiniLM-L6-v2',
        'SUPPORTED_MODELS': {
            'openai': ['gpt-4o-mini', 'gpt-4o', 'gpt-3.5-turbo'],
            'claude': ['claude-3-sonnet', 'claude-3-haiku', 'claude-3-opus']
        },
        'USE_GOOGLE_DRIVE': use_google_drive
    }

# í”„ë¡œì íŠ¸ ì„¤ì • ë¡œë“œ
SETTINGS = get_project_settings()

# ê°œë³„ ì„¤ì •ê°’ë“¤ì„ ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ export
BASE_DIR = SETTINGS['BASE_DIR']
PREP_JSON_FOLDER = SETTINGS['PREP_JSON_FOLDER']
VECTORSTORE_FOLDER = SETTINGS['VECTORSTORE_FOLDER']
PROMPT_TEMPLATES_FOLDER = SETTINGS['PROMPT_TEMPLATES_FOLDER']
MODEL_CACHE_FOLDER = SETTINGS['MODEL_CACHE_FOLDER']
LOGS_FOLDER = SETTINGS['LOGS_FOLDER']
EMBEDDING_MODEL = SETTINGS['EMBEDDING_MODEL']
SUPPORTED_MODELS = SETTINGS['SUPPORTED_MODELS']
USE_GOOGLE_DRIVE = SETTINGS['USE_GOOGLE_DRIVE']

# Model settings
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# LLM Configuration
LLM_CONFIG = {
    "openai": {
        "models": {
            "gpt-4o-mini": {
                "model_name": "gpt-4o-mini",
                "temperature": 0.1,
                "max_tokens": 2000,
                "description": "Fast and cost-effective model"
            },
            "gpt-4o": {
                "model_name": "gpt-4o",
                "temperature": 0.1,
                "max_tokens": 2000,
                "description": "Most capable model"
            },
            "gpt-3.5-turbo": {
                "model_name": "gpt-3.5-turbo",
                "temperature": 0.1,
                "max_tokens": 2000,
                "description": "Legacy model"
            }
        },
        "default_model": "gpt-4o-mini",
        "api_url": "https://api.openai.com/v1/chat/completions"
    },
    "claude": {
        "models": {
            "claude-3-sonnet": {
                "model_name": "claude-3-sonnet",
                "temperature": 0.1,
                "max_tokens": 2000,
                "description": "Balanced performance and speed"
            },
            "claude-3-haiku": {
                "model_name": "claude-3-haiku",
                "temperature": 0.1,
                "max_tokens": 2000,
                "description": "Fastest Claude model"
            },
            "claude-3-opus": {
                "model_name": "claude-3-opus",
                "temperature": 0.1,
                "max_tokens": 2000,
                "description": "Most capable Claude model"
            }
        },
        "default_model": "claude-3-sonnet",
        "api_url": "https://api.anthropic.com/v1/messages"
    }
}

# Vector store settings
VECTOR_STORE_CONFIG = {
    "collection_name": "datasheet_chunks",
    "embedding_model": EMBEDDING_MODEL,
    "chunk_size": CHUNK_SIZE,
    "chunk_overlap": CHUNK_OVERLAP,
    "use_gpu": True,  # Will fallback to CPU if GPU not available
}

# Streamlit settings
STREAMLIT_CONFIG = {
    "page_title": "ChipChat - ë°ì´í„°ì‹œíŠ¸ ì±—ë´‡",
    "page_icon": "ðŸ’¬",
    "layout": "wide",
    "initial_sidebar_state": "expanded",
}

# PDF processing settings (for reference)
PDF_PROCESSING_CONFIG = {
    "pages_per_chunk": 3,
    "categories": [
        "Product Summary",
        "Electrical Characteristics", 
        "Application Circuits",
        "Mechanical Characteristics",
        "Reliability and Environmental Conditions",
        "Packaging Information"
    ]
}

# Default prompt templates
DEFAULT_PROMPT_TEMPLATES = {
    "korean": {
        "pre": "ë‹¹ì‹ ì€ ì „ìž ë¶€í’ˆ ë°ì´í„°ì‹œíŠ¸ì— ëŒ€í•´ ì‘ë‹µí•˜ëŠ” ì „ë¬¸ ë„ìš°ë¯¸ìž…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
        "post": "ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´ ê·¸ ì ì„ ëª…ì‹œí•˜ì„¸ìš”."
    },
    "english": {
        "pre": "You are an expert assistant that answers questions about electronic component datasheets. Please provide accurate and detailed responses based on the provided context information.",
        "post": "Please provide a clear and concise answer based on the retrieved information. If the information is insufficient, please specify that."
    },
    "technical": {
        "pre": "ë‹¹ì‹ ì€ ì „ìžê³µí•™ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë°ì´í„°ì‹œíŠ¸ì˜ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ ì •í™•ížˆ ë¶„ì„í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        "post": "ê¸°ìˆ ì  ì •í™•ì„±ì„ ìš°ì„ ì‹œí•˜ì—¬ ë‹µë³€í•˜ê³ , ê´€ë ¨ ìˆ˜ì¹˜ë‚˜ ì‚¬ì–‘ì´ ìžˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”."
    }
}

# API Keys configuration
def get_api_keys() -> Dict[str, Optional[str]]:
    """Get API keys from environment variables or streamlit secrets"""
    api_keys = {
        'openai': os.environ.get('OPENAI_API_KEY'),
        'anthropic': os.environ.get('ANTHROPIC_API_KEY'), 
        'huggingface': os.environ.get('HF_TOKEN')
    }
    
    # Try to load from streamlit secrets if available
    try:
        import streamlit as st
        if hasattr(st, 'secrets'):
            api_keys['openai'] = api_keys['openai'] or st.secrets.get("openai_api_key")
            api_keys['anthropic'] = api_keys['anthropic'] or st.secrets.get("anthropic_api_key")
            api_keys['huggingface'] = api_keys['huggingface'] or st.secrets.get("hf_token")
    except ImportError:
        pass
    
    return api_keys

# Environment info
ENVIRONMENT = SETTINGS['USE_GOOGLE_DRIVE'] and 'colab' or 'local'

# Logging configuration
LOGGING_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "default": {
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "level": "INFO",
            "formatter": "default",
            "stream": "ext://sys.stdout"
        }
    },
    "root": {
        "level": "INFO",
        "handlers": ["console"]
    }
}

# Performance settings
PERFORMANCE_CONFIG = {
    "max_search_results": 50,
    "default_search_results": 5,
    "similarity_threshold": 0.7,
    "max_context_length": 4000,
    "enable_caching": True
}

# Feature flags
FEATURE_FLAGS = {
    "enable_metadata_display": True,
    "enable_source_tracking": True,
    "enable_chat_history": True,
    "enable_prompt_customization": True,
    "enable_multi_llm": True,
    "enable_filtering": True
} 