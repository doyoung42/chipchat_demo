import os
import shutil
import json
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime
import hashlib
import zipfile

from .logger import get_logger

class HFModelCache:
    """HuggingFace ëª¨ë¸ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        """
        Args:
            cache_dir: ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ë‚˜ í™˜ê²½ ê°ì§€ë¡œ ìë™ ì„¤ì •)
        """
        # í™˜ê²½ ê°ì§€
        self.is_colab = False
        if cache_dir is None:
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
            cache_dir = os.environ.get('MODEL_CACHE_DIR')
            
            if cache_dir is None:
                # í™˜ê²½ ê°ì§€ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
                try:
                    from google.colab import drive
                    # Google Colab í™˜ê²½
                    self.is_colab = True
                    cache_dir = "/content/drive/MyDrive/hf_model_cache"
                except ImportError:
                    # ë¡œì»¬ í™˜ê²½
                    self.is_colab = False
                    cache_dir = "./hf_model_cache"
        else:
            # cache_dirì´ Google Drive ê²½ë¡œì¸ì§€ í™•ì¸
            self.is_colab = "/content/drive" in str(cache_dir) or "/drive/MyDrive" in str(cache_dir)
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = get_logger()
        self.cache_metadata_file = self.cache_dir / "cache_metadata.json"
        self.cache_metadata = self._load_cache_metadata()
        
        # HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.hf_cache_dir = Path.home() / '.cache' / 'huggingface'
        
        # í™˜ê²½ì— ë”°ë¥¸ ë¡œê¹… ë©”ì‹œì§€ ì„¤ì •
        self.cache_storage_name = "Google Drive" if self.is_colab else "ë¡œì»¬ ìºì‹œ"
        
        self.logger.info("HF ëª¨ë¸ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", extra={
            "cache_dir": str(self.cache_dir),
            "hf_cache_dir": str(self.hf_cache_dir),
            "environment": "Colab" if self.is_colab else "Local"
        })
    
    def _load_cache_metadata(self) -> Dict[str, Any]:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if self.cache_metadata_file.exists():
            try:
                with open(self.cache_metadata_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.error(f"ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                return {}
        return {}
    
    def _save_cache_metadata(self):
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            with open(self.cache_metadata_file, 'w') as f:
                json.dump(self.cache_metadata, f, indent=2)
        except Exception as e:
            self.logger.error(f"ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _get_model_hash(self, model_name: str) -> str:
        """ëª¨ë¸ ì´ë¦„ì˜ í•´ì‹œê°’ ìƒì„±"""
        return hashlib.md5(model_name.encode()).hexdigest()[:8]
    
    @get_logger().measure_time("ëª¨ë¸ ìºì‹œ í™•ì¸")
    def is_model_cached(self, model_name: str) -> bool:
        """ëª¨ë¸ì´ ìºì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        model_hash = self._get_model_hash(model_name)
        
        if model_name in self.cache_metadata:
            cache_info = self.cache_metadata[model_name]
            cache_file = self.cache_dir / cache_info['filename']
            
            if cache_file.exists():
                self.logger.info(f"ëª¨ë¸ ìºì‹œ ë°œê²¬: {model_name}", extra={
                    "cache_file": str(cache_file),
                    "cached_at": cache_info.get('cached_at', 'unknown')
                })
                return True
        
        return False
    
    def save_model_to_cache(self, model_name: str, local_model_path: Optional[Path] = None):
        """ë¡œì»¬ HF ìºì‹œì˜ ëª¨ë¸ì„ ìºì‹œì— ì €ì¥"""
        # í™˜ê²½ì— ë”°ë¥¸ ë™ì  ë¡œê¹…
        operation_name = f"ëª¨ë¸ {self.cache_storage_name} ì €ì¥"
        
        @get_logger().measure_time(operation_name)
        def _save_operation():
            try:
                model_hash = self._get_model_hash(model_name)
                cache_filename = f"{model_name.replace('/', '_')}_{model_hash}.zip"
                cache_file_path = self.cache_dir / cache_filename
                
                # HuggingFace ìºì‹œì—ì„œ ëª¨ë¸ ì°¾ê¸°
                if local_model_path is None:
                    # transformers ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
                    transformers_cache = self.hf_cache_dir / 'hub'
                    if not transformers_cache.exists():
                        self.logger.error(f"HF ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {transformers_cache}")
                        return False
                    
                    # ëª¨ë¸ ê´€ë ¨ íŒŒì¼ë“¤ ì°¾ê¸°
                    model_files = []
                    model_name_safe = model_name.replace('/', '--')
                    
                    for item in transformers_cache.iterdir():
                        if model_name_safe in str(item):
                            model_files.append(item)
                    
                    if not model_files:
                        self.logger.error(f"ë¡œì»¬ ìºì‹œì—ì„œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_name}")
                        return False
                else:
                    model_files = [local_model_path]
                
                self.logger.info(f"ëª¨ë¸ íŒŒì¼ ì••ì¶• ì¤‘: {len(model_files)}ê°œ í•­ëª©")
                
                # ZIP íŒŒì¼ë¡œ ì••ì¶•
                with zipfile.ZipFile(cache_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file_path in model_files:
                        if file_path.is_file():
                            arcname = file_path.name
                            zipf.write(file_path, arcname)
                            self.logger.debug(f"ì••ì¶• ì¶”ê°€: {arcname}")
                        elif file_path.is_dir():
                            for root, dirs, files in os.walk(file_path):
                                for file in files:
                                    file_path_full = Path(root) / file
                                    arcname = str(file_path_full.relative_to(file_path.parent))
                                    zipf.write(file_path_full, arcname)
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                self.cache_metadata[model_name] = {
                    'filename': cache_filename,
                    'cached_at': datetime.now().isoformat(),
                    'size_bytes': cache_file_path.stat().st_size,
                    'model_hash': model_hash
                }
                self._save_cache_metadata()
                
                self.logger.info(f"ëª¨ë¸ ìºì‹œ ì €ì¥ ì™„ë£Œ: {model_name}", extra={
                    "cache_file": str(cache_file_path),
                    "size_mb": f"{cache_file_path.stat().st_size / 1024 / 1024:.2f}"
                })
                
                # ì‚¬ìš©ìì—ê²Œ ì €ì¥ ìœ„ì¹˜ í‘œì‹œ
                size_mb = cache_file_path.stat().st_size / 1024 / 1024
                print(f"âœ… ëª¨ë¸ì´ {self.cache_storage_name}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {cache_file_path}")
                print(f"ğŸ’¾ íŒŒì¼ í¬ê¸°: {size_mb:.2f} MB")
                
                return True
                
            except Exception as e:
                self.logger.error(f"ëª¨ë¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
        
        return _save_operation()
    
    def load_model_from_cache(self, model_name: str) -> bool:
        """ìºì‹œì—ì„œ ëª¨ë¸ì„ ë¡œì»¬ HF ìºì‹œë¡œ ë³µì›"""
        # í™˜ê²½ì— ë”°ë¥¸ ë™ì  ë¡œê¹…
        operation_name = f"ëª¨ë¸ {self.cache_storage_name} ë¡œë“œ"
        
        @get_logger().measure_time(operation_name)
        def _load_operation():
            try:
                if model_name not in self.cache_metadata:
                    self.logger.error(f"ìºì‹œ ë©”íƒ€ë°ì´í„°ì— ëª¨ë¸ì´ ì—†ìŒ: {model_name}")
                    return False
                
                cache_info = self.cache_metadata[model_name]
                cache_file_path = self.cache_dir / cache_info['filename']
                
                if not cache_file_path.exists():
                    self.logger.error(f"ìºì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {cache_file_path}")
                    return False
                
                # HuggingFace ìºì‹œ ë””ë ‰í† ë¦¬ ì¤€ë¹„
                transformers_cache = self.hf_cache_dir / 'hub'
                transformers_cache.mkdir(parents=True, exist_ok=True)
                
                self.logger.info(f"ìºì‹œ íŒŒì¼ ì••ì¶• í•´ì œ ì¤‘: {cache_file_path}")
                
                # ZIP íŒŒì¼ ì••ì¶• í•´ì œ
                with zipfile.ZipFile(cache_file_path, 'r') as zipf:
                    zipf.extractall(transformers_cache)
                
                self.logger.info(f"ëª¨ë¸ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {model_name}", extra={
                    "cache_file": str(cache_file_path),
                    "target_dir": str(transformers_cache)
                })
                
                # ì‚¬ìš©ìì—ê²Œ ë¡œë“œ ì™„ë£Œ ì•Œë¦¼
                print(f"âœ… ëª¨ë¸ì„ {self.cache_storage_name}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ“ ë¡œë“œ ìœ„ì¹˜: {transformers_cache}")
                
                return True
                
            except Exception as e:
                self.logger.error(f"ëª¨ë¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
        
        return _load_operation()
    
    def get_cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ ì •ë³´ ë°˜í™˜"""
        total_size = 0
        model_count = len(self.cache_metadata)
        
        for model_info in self.cache_metadata.values():
            if 'size_bytes' in model_info:
                total_size += model_info['size_bytes']
        
        return {
            'cache_dir': str(self.cache_dir),
            'model_count': model_count,
            'total_size_mb': total_size / 1024 / 1024,
            'models': list(self.cache_metadata.keys())
        }
    
    def clear_cache(self, model_name: Optional[str] = None):
        """ìºì‹œ ì‚­ì œ"""
        try:
            if model_name:
                # íŠ¹ì • ëª¨ë¸ë§Œ ì‚­ì œ
                if model_name in self.cache_metadata:
                    cache_info = self.cache_metadata[model_name]
                    cache_file = self.cache_dir / cache_info['filename']
                    
                    if cache_file.exists():
                        cache_file.unlink()
                    
                    del self.cache_metadata[model_name]
                    self._save_cache_metadata()
                    
                    self.logger.info(f"ëª¨ë¸ ìºì‹œ ì‚­ì œ: {model_name}")
            else:
                # ì „ì²´ ìºì‹œ ì‚­ì œ
                for cache_file in self.cache_dir.glob("*.zip"):
                    cache_file.unlink()
                
                self.cache_metadata.clear()
                self._save_cache_metadata()
                
                self.logger.info("ì „ì²´ ëª¨ë¸ ìºì‹œ ì‚­ì œ ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
_model_cache_instance = None

def get_model_cache(cache_dir: Optional[str] = None) -> HFModelCache:
    """ì‹±ê¸€í†¤ ëª¨ë¸ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _model_cache_instance
    
    if _model_cache_instance is None:
        if cache_dir is None:
            # config.jsonì—ì„œ ìºì‹œ ë””ë ‰í† ë¦¬ ì½ì–´ì˜¤ê¸°
            try:
                from .config_manager import get_config_manager
                config = get_config_manager()
                cache_dir = config.get_path('model_cache_folder')
            except Exception:
                # config.json ì½ê¸° ì‹¤íŒ¨ ì‹œ í™˜ê²½ ê°ì§€ë¡œ í´ë°±
                try:
                    from google.colab import drive
                    # Google Colab í™˜ê²½
                    cache_dir = "/content/drive/MyDrive/hf_model_cache"
                except ImportError:
                    # ë¡œì»¬ í™˜ê²½
                    cache_dir = "./hf_model_cache"
        
        _model_cache_instance = HFModelCache(cache_dir)
    
    return _model_cache_instance 